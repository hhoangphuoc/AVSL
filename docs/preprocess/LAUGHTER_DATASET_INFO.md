# AMI Laughter Dataset Processing

This document describes how to create a HuggingFace dataset for laughter events and fluent speech segments from the AMI Meeting Corpus using the `laugh_dataset_process.py` script.

## Overview

The script processes the AMI Meeting Corpus to extract specific segments marked as either "laughter" or "fluent" speech based on the annotations in `ami_laugh_markers.csv`. Each segment is:
- Extracted from the original audio and video files
- Optionally processed to extract lip regions from the video
- Organized into a HuggingFace dataset with comprehensive metadata

## Dataset Structure

Each dataset record contains:
- `segment_id`: Unique identifier for the segment
- `meeting_id`: AMI meeting identifier (e.g., "ES2002a")
- `speaker_id`: Speaker identifier (A, B, C, D, or E)
- `transcript`: The word(s) in that segment
- `disfluency_type`: Either "laughter" or "fluent"
- `audio`: HuggingFace Audio object (or None if unavailable)
- `lip_video`: HuggingFace Video object of lip region (or None if unavailable)
- `audio_path`: Path to the audio file
- `video_path`: Path to the video file
- `start_time`: Start timestamp in seconds
- `end_time`: End timestamp in seconds
- `duration`: Duration of the segment

## Prerequisites

1. The AMI Meeting Corpus should be available at the path specified in `constants.py`
2. The `ami_laugh_markers.csv` file should exist (generated by `disfluency_laughter_process.py`)
3. Required Python packages (from requirements.txt)
4. Face detection models (will be downloaded automatically if not present)

## Usage

### Basic Usage

```bash
python preprocess/laugh_dataset_process.py
```

This will use default settings to process the dataset.

### Advanced Usage

```bash
python preprocess/laugh_dataset_process.py \
    --csv_path /path/to/ami_laugh_markers.csv \
    --output_dir /path/to/output/segments \
    --dataset_path /path/to/huggingface/dataset \
    --extract_lip_videos \
    --to_grayscale \
    --batch_size 16 \
    --use_shards \
    --files_per_shard 2000
```

### Command Line Arguments

- `--csv_path`: Path to the ami_laugh_markers.csv file (default: `./ami_laugh_markers.csv`)
- `--output_dir`: Directory to save processed audio/video segments (default: `DATA_PATH/laughter_dataset/segments`)
- `--dataset_path`: Path to save the HuggingFace dataset (default: `DATA_PATH/laughter_dataset/dataset`)
- `--extract_lip_videos`: Extract lip regions from videos (default: True)
- `--no_lip_videos`: Skip lip video extraction
- `--to_grayscale`: Convert lip videos to grayscale for efficiency (default: True)
- `--batch_size`: Batch size for lip extraction processing (default: 8)
- `--use_shards`: Use sharded dataset format for large datasets (default: True)
- `--no_shards`: Use standard dataset format
- `--files_per_shard`: Number of media files per shard (default: 2000)

## Processing Steps

1. **Load Markers**: Reads the CSV file containing laughter and fluent speech annotations
2. **Filter Segments**: Keeps only "laughter" and "fluent" segments
3. **Group by Source**: Groups segments by their source audio/video files for efficient batch processing
4. **Extract Audio**: Segments audio files at 16kHz sampling rate
5. **Extract Video**: Segments video files at 25fps in MP4 format
6. **Extract Lip Regions**: Processes videos to extract 96x96 lip regions (optional)
7. **Create Dataset**: Assembles all data into a HuggingFace dataset

## Output Structure

```
output_dir/
├── audio_segments/         # Extracted audio segments (.wav files)
├── video_segments/         # Extracted video segments (.mp4 files)
├── lip_videos/            # Extracted lip region videos (.mp4 files)
└── dataset_records.json   # Complete dataset metadata

dataset_path/
├── data/                  # Media files (potentially sharded)
│   ├── shard_0000/
│   ├── shard_0001/
│   └── ...
├── dataset_dict.json      # HuggingFace dataset structure
├── metadata.jsonl         # Metadata for all records
└── ami_laughter-segmented-info.csv  # CSV with all segment information
```

## Performance Considerations

- **Memory Usage**: The script processes files in batches to minimize memory usage
- **Disk Space**: Ensure sufficient disk space for:
  - Audio segments: ~1GB per 1000 segments
  - Video segments: ~5GB per 1000 segments
  - Lip videos: ~2GB per 1000 segments
- **Processing Time**: 
  - Audio/video extraction: ~10-20 segments per second
  - Lip extraction: ~1-2 segments per second (depending on GPU)

## Troubleshooting

### Common Issues

1. **Missing Source Files**: If audio/video files are not found, segments will be skipped
2. **Face Detection Failures**: Some videos may not have detectable faces, lip extraction will fail gracefully
3. **Memory Issues**: Reduce `batch_size` or process in smaller chunks
4. **Disk Space**: Ensure sufficient space before processing

### Error Messages

- `"Speaker X not found in mapping"`: Speaker ID not recognized (should be A-E)
- `"No face detected"`: Face detection failed for a video segment
- `"Invalid video segment"`: Start time >= end time or negative timestamps

## Example Script for Processing in Chunks

If processing the entire dataset at once is too memory-intensive:

```python
import pandas as pd

# Load and split the CSV
df = pd.read_csv('ami_laugh_markers.csv')
df_filtered = df[df['disfluency_type'].isin(['laughter', 'fluent'])]

# Process by meeting
for meeting_id in df_filtered['meeting_id'].unique():
    meeting_df = df_filtered[df_filtered['meeting_id'] == meeting_id]
    meeting_df.to_csv(f'temp_{meeting_id}.csv', index=False)
    
    # Process this meeting
    os.system(f"""
        python laugh_dataset_process.py \
            --csv_path temp_{meeting_id}.csv \
            --output_dir output/{meeting_id} \
            --dataset_path dataset/{meeting_id}
    """)
```

## Integration with Existing Pipelines

The generated dataset is compatible with:
- HuggingFace Datasets library
- PyTorch DataLoader (via HuggingFace)
- Direct loading for model training

Example loading code:
```python
from datasets import load_from_disk

# Load the dataset
dataset = load_from_disk('/path/to/dataset')

# Access a sample
sample = dataset[0]
print(f"Segment: {sample['segment_id']}")
print(f"Type: {sample['disfluency_type']}")
print(f"Transcript: {sample['transcript']}")

# Audio is automatically loaded as numpy array
if sample['audio'] is not None:
    audio_array = sample['audio']['array']
    sampling_rate = sample['audio']['sampling_rate']
```
