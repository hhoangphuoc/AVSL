train_name: whisper_flamingo_large_ft_ami

# Data paths for Hugging Face datasets (must be loadable by load_from_disk)
# NOTE: This is the path to the validated videos
train_data_path: "/home/s2587130/AVSL/data/ami/av_hubert/train_clean"  
val_data_path: "/home/s2587130/AVSL/data/ami/av_hubert/validation_clean"
test_data_path: "/home/s2587130/AVSL/data/ami/av_hubert/test_clean"
# NOTE: This is the path to the original videos 
train_data_path_original: "/home/s2587130/AVSL/data/ami/av_hubert/train"
val_data_path_original: "/home/s2587130/AVSL/data/ami/av_hubert/validation"
test_data_path_original: "/home/s2587130/AVSL/data/ami/av_hubert/test"

# Max audio length for LengthBatchSampler (in samples, e.g., 15s * 16000 = 240000)
# This is different from dataset_audio_max_length for pad_or_trim
audio_max_length: 240000 

# Max audio length for whisper.pad_or_trim (in samples, e.g., 30s * 16000 = 480000)
# Defaults to whisper.audio.N_SAMPLES (480000) if not set in script.
dataset_audio_max_length: 480000

# Max duration in seconds to filter dataset samples before training/evaluation
max_duration_filter_seconds: 30.0

text_max_length: 350 # Max text length (not directly used by AmiVideoHFDataset, but good for reference)
accelerator: auto
weight_decay: 0.01
adam_epsilon: 1.0e-8
num_worker: 16 # Number of workers for DataLoader
validate_every_n_batches: 1000
num_devices: 1

model_name: large-v2 # Whisper model size
learning_rate: 1.0e-5 # Adjusted typical LR for fine-tuning large models

batch_size: 2 # Reduced batch size for large model to avoid OOM
eval_batch_size: 4 # Batch size for evaluation dataloaders

num_train_steps: 8000 #20000 # Example, adjust based on dataset size and epochs
warmup_steps: 1000 #5000
gradient_accumulation_steps: 4 # Increased to maintain effective batch size

monitor: 'val/wer_av' # Monitor WER on the validation set for AV model (now only one val set)

video_model_ckpt: '/home/s2587130/AVSL/avsl/models/large_noise_pt_noise_ft_433h_only_weights.pt' # Path to AV-HuBERT weights
freeze_video_model: True
freeze_video_batch_norm_stats: False 

spec_augment: "ls-basic" # SpecAugment config for AmiVideoHFDataset ("ls-double", "ls-basic", or None)
dropout_rate: 0.1 # Dropout rate in Whisper model

lang: en # Language is English for AMI dataset
pt_ckpt: '/home/s2587130/AVSL/avsl/models/whisper_en_large.pt' # Path to pre-trained audio-only Whisper checkpoint (optional)
resume_training: False
train_id: whisper-flamingo_large_ft_ami # Updated train_id

video_projection_train_only: False
video_projection_separate_lr: '' # Not used
prob_use_av: 1.0 # For Whisper-Flamingo, no modality dropout in this config

# Video settings
use_av_hubert_encoder: True
add_gated_x_attn: 1 # 0 for False, 1 for True (enable Flamingo-style cross-attention)
av_fusion: separate # AV fusion strategy

log_output_dir: "/home/s2587130/AVSL/avsl/output/train_whisper_flamingo_ft" # Directory for TensorBoard logs
check_output_dir: "/home/s2587130/AVSL/avsl/checkpoints/whisper_flamingo_ft"   # Directory for model checkpoints

num_sanity_val_steps: 2 # Number of sanity check batches before training
precision: 16 # Mixed precision training (16 or bf16)
reload_dataloaders_every_n_epochs: 1
sync_batchnorm: True

# Use local model directory instead of downloading
download_root: '/home/s2587130/AVSL/avsl/models/whisper' # Local directory where models are stored
# # Option to load from a specific local model file (bypasses download)
# local_whisper_model: '/home/s2587130/AVSL/avsl/models/whisper_en_large.pt' # Use existing model file 